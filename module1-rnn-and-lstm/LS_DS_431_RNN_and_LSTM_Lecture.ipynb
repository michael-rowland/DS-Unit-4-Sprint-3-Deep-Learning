{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 19s 772us/sample - loss: 0.2453 - accuracy: 0.9046 - val_loss: 0.3850 - val_accuracy: 0.8372\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 19s 768us/sample - loss: 0.2241 - accuracy: 0.9141 - val_loss: 0.3837 - val_accuracy: 0.8391\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=1024, \n",
    "          epochs=2, \n",
    "          validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some recent headlines from schools around the country: In Indiana, officials played a segment of a 911 call of a teacher in a panic during the Columbine High School shooting to students. In Ohio, officers fired blank shots during an active-shooter drill. In South Carolina, an officer dressed in black posed as an intruder on an unannounced drill. In Michigan, a school is spending $48 million on a renovation that includes curved hallways and hiding niches, in hopes of protecting students from a mass shooting. In Florida, a police officer arrested two 6-year-old students for misdemeanor battery. In Colorado, teachers received buckets and kitty litter for students to use as toilets in case of a prolonged school lockdown.\\n\\nMass shootings, meaning incidents with at least two deaths, in schools are horrifying. But it is highly unlikely that a child would ever witness one. Research indicates that some security measures brought in to make schools safer — like realistic shooter trainings — may be causing children more harm than good.\\n\\nIt is 10 times more likely that a student will die on the way to school.\\n\\nOur chances of dying in a fire are also much greater — 1 in 1,500. But we don’t overreact.\\n\\nMore children have died from lightning strikes than from mass shootings in schools in the past 20 years. Still, we don’t obsess about them.\\n\\nExactly how common are school shootings?\\n\\nIn the two decades since Columbine, there have been 10 mass shootings in schools according to a recent analysis by James Alan Fox, a professor of criminology at Northeastern University who has been studying school violence for several decades. In total, 81 people have been killed, 64 of them students. That’s an average of four deaths per year, three of them students.\\n\\nEven one death is too many. But for perspective, 729 children committed suicide with a firearm in 2017, and 863 were victims of homicides by guns that year.\\n\\nSchool-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides in 2017 Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University.\\n\\nNearly every public school in the country now conducts lockdown drills, and even the youngest students participate (last year, one school adapted a lullaby to prepare kindergartners). But very few studies have looked into the efficiency of these drills. One of them concluded that the practice can be helpful to teach students basic safety procedures. But to the author of the study, Jaclyn Schildkraut, an associate professor at the State University of New York at Oswego, there is no point in dramatizing the drills. “All that causes is fear,” she said.\\n\\nRestaurants have 10 times as many homicides as schools. Why do we want to arm teachers and not wait staffs?\\n\\n“There’s a misunderstanding in where the dangers are,” said Dewey G. Cornell, a psychologist and professor at the University of Virginia. “Kids are at far greater danger going to and from school, than they are in the classroom,” he said. “School counseling, academic support, that’s gonna do far more to keep our communities safe.”\\n\\nUnlike the United States, the other wealthy countries in the Group of Seven don’t do lockdown drills and rarely have school shootings. What is the United States doing that is so different from them?\\n\\nGun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017.\\n\\nMany researchers think easy access to guns is an important part of the problem. “Violence in schools is just a small part of the larger problem of gun violence in our society,” Cornell wrote in a statement about prevention of violence in schools and communities.\\n\\nMisguided safety measures, such as dramatized lockdown drills, may give us the impression that we are protecting children, when, in fact, we are handing them a burden that adults are failing to address.\\n\\nRead more:\\n\\n‘What if someone was shooting?’\\n\\nThey grew up practicing lockdown drills. Now they’re steering the conversation on gun violence.\\n\\nSchool shootings are extraordinarily rare. Why is fear of them driving policy?\\n\\nPutting more cops in schools won’t make schools safer, and it will likely inflict a lot of harm'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 99,\n",
       " 118,\n",
       " 21,\n",
       " 15,\n",
       " 107,\n",
       " 4,\n",
       " 24,\n",
       " 21,\n",
       " 107,\n",
       " 118,\n",
       " 54,\n",
       " 70,\n",
       " 55,\n",
       " 99,\n",
       " 88,\n",
       " 24,\n",
       " 61,\n",
       " 118,\n",
       " 107,\n",
       " 89,\n",
       " 21,\n",
       " 87,\n",
       " 87,\n",
       " 46,\n",
       " 69,\n",
       " 10,\n",
       " 70,\n",
       " 16,\n",
       " 99,\n",
       " 24,\n",
       " 89,\n",
       " 10,\n",
       " 70,\n",
       " 107,\n",
       " 89,\n",
       " 70,\n",
       " 99,\n",
       " 118,\n",
       " 70]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contributing columnist\\n\\nThe House is on fire.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_char[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), dropout=.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 3.2714\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"e repercussions for the West, especially\"\n",
      "e repercussions for the West, especiallyoeae oosu  o tmuwirn“o t,rr  unvoA,e m ma,a  e rolli\n",
      "yieiutv ehSfaion atfos e ponTedatanaarieit \n",
      "ostAWioayraodhelarnou ins eU  et faocR to da nn eaof ryos.oIchao.ftn scsgpvtym idod  laorl”acc Soo-t ’ioUor sinstieiak sf—eDw:henhef, Ja,s Gjesiceisnot“n ol, aoar”otes woraatu d.0licn d whaig Thg’r elwdel\n",
      "rheAs ámveennasfHe s ,r.dyaesce en\n",
      " uud kd ..fidiestrr s iy ooiiiieahOiCstah mi hteoa vr?gbem  ii \n",
      "178374/178374 [==============================] - 67s 375us/sample - loss: 3.2712\n",
      "Epoch 2/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.9828\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"eing released,” Nichols said, knowing th\"\n",
      "eing released,” Nichols said, knowing the ne anYenotan rcrRrtF re dhan tnennKed’puns  mecdnt orgend efamu aTBt  osv iviits s ganahas ret  a/s Aiadeet ond,ains un, y\n",
      " us id lpanuf shbnltAátsssox bennnN sere dn Cha ner stirTre Msl isennglan Aon wet la, tracaes \n",
      "an aoaicte chces Vlgerhei f ps thicran titin Cr onge toes venges hd 'rnA5ehe, \n",
      "ievn sot,eu w bAeh t\n",
      "r,r pn wind i. myevxpe hi  o bar Nandreif heef un yuid.rtte pipglte aigirkh ir t\n",
      "178374/178374 [==============================] - 72s 405us/sample - loss: 2.9827\n",
      "Epoch 3/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.7707\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"But Airbnb says the bill is nothing more\"\n",
      "But Airbnb says the bill is nothing more wont cattsmalat verzsed lutits cat acum0 tiy pher aa nothir toli. Drilsale lofaxgicey elentac an mimesd  aeeved thr, Vw te mIoo, eadlfinnant tuasdhttateso tes tharmsee te Hicsulh pverkin sy , 1svH aseuitocint oottfo’cse he ass s ority er hove !p ef pen coonth .u tekthemd eotle shnrendwd fftenabr,.r“sonving lell ser td ab ine sbothed to gres berekir onj wufytu; lee lous,touuokl therisesn: pwThe  i\n",
      "178374/178374 [==============================] - 77s 434us/sample - loss: 2.7704\n",
      "Epoch 4/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.6717\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"ears in New York and knew Petit as a uni\"\n",
      "ears in New York and knew Petit as a uniblaoG aiskerhioupoesdiceuve to mhepe-lt ans ouk  lome inlebrdaar “wefat ha, vere,es hkd Hheaproedy heal’n, touca alydhe ding Loukr” \n",
      "ast aryFbrwies d\n",
      "aH.injlt ram lepcredy th sh’ innt:nter ao :e -ormyaprHg o iceht kit pat okith a moran aot taasgsod taa hhice resig woa  \n",
      "1:e fumpecithing eon verpsice ind Ding 1om vory Thacupms mloxicd the to yAmis Cafdane’, Conpalibothete:y\n",
      "Dtinw an un se s tir anc\n",
      "178374/178374 [==============================] - 76s 429us/sample - loss: 2.6715\n",
      "Epoch 5/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.6156\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \" like to say that breast-feeding, and be\"\n",
      " like to say that breast-feeding, and beilsdensealle sWbir, nnbitung se the tta tice hing_ace at oetini xeaenwht mer oo ciceckld D” wmost att acistor shau the bamegons-ny anywCin egsind thir8 “hass Sertinda”her Ass r” ch kchd Ss0meithene Mentide durcKRle sheet tall. $n sofroms ind on dan fiau: (h.\n",
      "\n",
      "hvN Sist nrebcane Semuce the te curis ordinpt e taceot and onvblle soce theswy tat Socppme ar id 8o0 twas ma Uin porkacr aave hump fouse , a\n",
      "178374/178374 [==============================] - 80s 449us/sample - loss: 2.6155\n",
      "Epoch 6/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5755\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"President Emmanuel Macron that he was op\"\n",
      "President Emmanuel Macron that he was op ulyed’s out forlingit tofe\n",
      "Atc Sfar ans alsiwe ohed ool 1591iytiss arecowiahe hefenstrkenon ipsaly\n",
      "mREn Recoicreegr wallibs borbacas footw dicaverendage sisce arery ty tir ane en Speadtong haicane ho. TEumfzsto “óho talis harimigtiin”\n",
      "\n",
      "Ae whid th” ron?e,th Cfal RoreUlecacress\n",
      "\n",
      "\n",
      "Dand mrSite onderley forks, an anp asecilss. Stis ore s a-s— .\n",
      "Top blon of an thy whe ile, fars a-mon’ /s mfassiugnteger\n",
      "178374/178374 [==============================] - 77s 431us/sample - loss: 2.5756\n",
      "Epoch 7/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5418\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"are posted you will be deemed to have ac\"\n",
      "are posted you will be deemed to have actid puslay ond fat ha. alyocee ibe ,umhe an-ergat rochacs ead fedewit la sediog apicant SR bule _rcogeesosinin, Ar fxanit d nhint \n",
      " whore quedepucn toby hoin’s tne ches ing inns ohl jesthit stSutautfqus ar nince to prigens irvisiVky serectia ilye ul at  romeare oeroe ptoth” ouldiatede siso r bousr ath ghit sier ho kgeshtrunisrn2. Tiem sor ur hhettting itr oe ano hesebcing jo erther treikisu tisn.\n",
      "\n",
      "178374/178374 [==============================] - 78s 437us/sample - loss: 2.5416\n",
      "Epoch 8/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5193\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"iami, according to the New York Times, w\"\n",
      "iami, according to the New York Times, watt on9ing cubha urs das at ou elif arcthensiofdoThill fad ha fog pbokm on (in fount tamKeycen Ramed ordongs. TBurrieg hian mendecb bens , palven ate mpecesucscive hilnt seagsd dyorg Jan ressera t ow ay “reweed to Sthal oy rern ail war mallibl tot vighil whicafent’  ine As as anfto to rones wacfed ou — 26varsad ot the Eueclu\" R asticegas ute cuime ands no toreraletino thobswin mffipp— poticg tn. t\n",
      "178374/178374 [==============================] - 79s 446us/sample - loss: 2.5191\n",
      "Epoch 9/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4987\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"alth insurance coverage through a Market\"\n",
      "alth insurance coverage through a Market ant ont titht fcos arsirg a onamiisits tef. us mas.ssodrod do-iciCstaicuyn ouactceud thatring gh caid xhetu to ath ghedeteanJsud sirbich moyencor2 arminh cofitinw borst ar aselicn interscmeon Rosta.eTLarar Tiutig.”say ansi, wasnonn I Mongene, Bidure”.\n",
      "\n",
      "fite ornsing aunh CoUs\"ed aledanio sertich mutman theco kicdur who Seefstel apun. Bhen ios-an’s bn yoxen xark’o and.\n",
      "\n",
      "MStangt comle ocendabit— Mos\n",
      "178374/178374 [==============================] - 81s 457us/sample - loss: 2.4986\n",
      "Epoch 10/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4792\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"rticipating justices appointed by Nixon \"\n",
      "rticipating justices appointed by Nixon tf thatk]\n",
      "\n",
      "\n",
      "11l atllea Nilcer, hiat lata ses iv ing aby w more, the erpur” og oads wete Moritor owe inrwitat gemwtrtibecamice.\n",
      " wor denh Perit achereekas risila't aryitou, the Uatol sofpTirk cos tord tere$Ivecial\n",
      "\n",
      "An, ab suld coatith, andes, hapeibecobee’t tatDe ioner Le 20 hopl.\n",
      "\n",
      "This for chit, al onvame st they in layd beranh whe rinas.\n",
      "Ah.\n",
      "\n",
      "Wy vin te anseno whe lssent wh sulis uss mower fios fo\n",
      "178374/178374 [==============================] - 82s 460us/sample - loss: 2.4791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc739de5320>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=1024,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S3-MNA-DS11",
   "language": "python",
   "name": "u4-s3-mna-ds11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
